{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1a3fbff",
   "metadata": {},
   "source": [
    "# AI-Augmented Causal Inference for Active Transportation\n",
    "\n",
    "This tutorial demonstrates the AI-augmented causal inference approach described in Section 5 of the paper \"Causality in Active Transportation: Exploring Travel Behavior and Well-being\". We'll implement the three-layer architecture (Φ → Ψ → Γ) that converts high-dimensional, multimodal transportation data into semiparametrically identified causal contrasts.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The three-layer architecture consists of:\n",
    "1. **Representation Learning (Φ)**: Embeds heterogeneous inputs (images, GPS traces, text) into a latent feature vector\n",
    "2. **Balancing (Ψ)**: Outputs stabilized weights to equate treated and control distributions in the latent space\n",
    "3. **Causal Learning (Γ)**: Produces orthogonal scores or influence-function corrections for robust causal estimation\n",
    "\n",
    "We'll focus on implementing these components for active transportation research, demonstrating how they can be used to analyze the causal effect of pedestrian-friendly infrastructure on travel behavior and well-being."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703fda01",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "First, let's import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f561963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Deep learning libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Import CISD package\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from cisd.representation import RepresentationLearner, StreetviewEncoder, TextEncoder, MultiModalEncoder\n",
    "from cisd.balancing import Balancer, KernelMMD\n",
    "from cisd.causal import CausalLearner, DoublyRobust\n",
    "from cisd.ai_pipeline import ThreeLayerArchitecture, ActiveBERTDML\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb1d20",
   "metadata": {},
   "source": [
    "## 1. Generating Synthetic Multimodal Transportation Data\n",
    "\n",
    "For this tutorial, we'll generate synthetic data that simulates the kind of multimodal data found in active transportation research:\n",
    "\n",
    "1. **Street View Images**: Representations of the built environment (sidewalks, bike lanes, etc.)\n",
    "2. **GPS-Accelerometer Traces**: Movement patterns and physical activity\n",
    "3. **Textual Data**: Comments or descriptions of commuting experiences\n",
    "\n",
    "For simplicity, we'll use simplified synthetic versions of these data modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153566aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_multimodal_data(n_samples=1000):\n",
    "    \"\"\"Generate synthetic multimodal transportation data.\"\"\"\n",
    "    \n",
    "    # Generate baseline covariates\n",
    "    ses = np.random.normal(0, 1, n_samples)  # Socioeconomic status\n",
    "    distance = np.random.gamma(2, 2, n_samples)  # Distance to work/school in km\n",
    "    age = np.random.normal(40, 10, n_samples)  # Age in years\n",
    "    gender = np.random.binomial(1, 0.5, n_samples)  # 0: male, 1: female\n",
    "    walkability = 0.3*ses + 0.2*np.random.normal(0, 1, n_samples)  # Neighborhood walkability\n",
    "    \n",
    "    # Combine covariates\n",
    "    X = np.column_stack([ses, distance, age, gender, walkability])\n",
    "    \n",
    "    # Generate treatment assignment (PFIP exposure)\n",
    "    propensity = 1 / (1 + np.exp(-(0.5 + 0.7*ses + 0.3*walkability - 0.2*distance)))\n",
    "    D = np.random.binomial(1, propensity)\n",
    "    \n",
    "    # Generate outcome (well-being)\n",
    "    Y = 0.2*ses + 0.1*age - 0.2*distance + 0.3*walkability + 0.5*D + 0.3*np.random.normal(0, 1, n_samples)\n",
    "    Y = (Y - np.mean(Y)) / np.std(Y)  # Standardize\n",
    "    \n",
    "    # Generate synthetic streetview images (simplified as 32x32 RGB images)\n",
    "    # In reality, these would be actual street images\n",
    "    images = np.zeros((n_samples, 32, 32, 3))\n",
    "    \n",
    "    # PFIP areas have more sidewalk features (represented as higher blue channel values)\n",
    "    for i in range(n_samples):\n",
    "        # Base image with noise\n",
    "        img = np.random.rand(32, 32, 3) * 0.2\n",
    "        \n",
    "        # Add \"sidewalk\" features based on treatment\n",
    "        if D[i] == 1:\n",
    "            # PFIP areas: more sidewalk/bike lane features\n",
    "            # Add horizontal lines representing sidewalks\n",
    "            sidewalk_pos = np.random.randint(5, 27)\n",
    "            img[sidewalk_pos:sidewalk_pos+5, :, 2] += 0.6  # Blue channel for sidewalks\n",
    "            \n",
    "            # Add vertical lines for crosswalks\n",
    "            for j in range(0, 32, 8):\n",
    "                img[:, j:j+2, 2] += 0.4\n",
    "                \n",
    "            # Add green space\n",
    "            img[0:5, :, 1] += 0.5  # Green channel for trees/plants\n",
    "        else:\n",
    "            # Non-PFIP areas: fewer pedestrian features\n",
    "            # Narrower sidewalks\n",
    "            sidewalk_pos = np.random.randint(10, 25)\n",
    "            img[sidewalk_pos:sidewalk_pos+2, :, 2] += 0.3\n",
    "            \n",
    "            # More road space (red/gray)\n",
    "            img[5:25, :, 0] += 0.4  # Red channel for roads\n",
    "        \n",
    "        images[i] = np.clip(img, 0, 1)  # Ensure values are in [0,1]\n",
    "    \n",
    "    # Generate synthetic GPS-accelerometer traces\n",
    "    # We'll represent these as sequences of (lat, lon, acceleration) over 24 hours with hourly samples\n",
    "    gps_traces = np.zeros((n_samples, 24, 3))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # Create a daily mobility pattern\n",
    "        base_lat = np.random.normal(0, 1)\n",
    "        base_lon = np.random.normal(0, 1)\n",
    "        \n",
    "        for hour in range(24):\n",
    "            if 7 <= hour <= 9:  # Morning commute\n",
    "                movement = 0.5 + 0.2 * np.random.rand()\n",
    "                accel = 0.7 + 0.3 * np.random.rand()\n",
    "            elif 16 <= hour <= 19:  # Evening commute\n",
    "                movement = 0.4 + 0.2 * np.random.rand()\n",
    "                accel = 0.6 + 0.3 * np.random.rand()\n",
    "            elif 9 < hour < 16:  # Work hours\n",
    "                movement = 0.1 + 0.1 * np.random.rand()\n",
    "                accel = 0.2 + 0.2 * np.random.rand()\n",
    "            else:  # Night\n",
    "                movement = 0.05 + 0.05 * np.random.rand()\n",
    "                accel = 0.1 + 0.1 * np.random.rand()\n",
    "            \n",
    "            # Active transportation is associated with more varied acceleration patterns\n",
    "            if D[i] == 1:  # PFIP areas have more active transportation\n",
    "                accel_mod = accel * (1.0 + 0.5 * np.random.rand())\n",
    "            else:\n",
    "                accel_mod = accel\n",
    "                \n",
    "            gps_traces[i, hour, 0] = base_lat + movement * np.random.randn()\n",
    "            gps_traces[i, hour, 1] = base_lon + movement * np.random.randn()\n",
    "            gps_traces[i, hour, 2] = accel_mod\n",
    "    \n",
    "    # Generate textual comments about commuting experience\n",
    "    positive_comments = [\n",
    "        \"Wide sidewalks make walking comfortable and safe.\",\n",
    "        \"I enjoy my commute with the new bike lanes.\",\n",
    "        \"The crosswalks are well-marked and drivers stop for pedestrians.\",\n",
    "        \"Beautiful trees along the path make walking pleasant.\",\n",
    "        \"Traffic calming measures have really improved walking safety.\",\n",
    "        \"I feel secure walking after the street lighting was improved.\",\n",
    "        \"The separated bike lanes keep me safe from traffic.\",\n",
    "        \"Well-designed pedestrian signals give enough time to cross safely.\",\n",
    "        \"The neighborhood is very walkable since the improvements.\",\n",
    "        \"Walking to work is now part of my daily exercise routine.\"\n",
    "    ]\n",
    "    \n",
    "    negative_comments = [\n",
    "        \"No sidewalks make walking dangerous in my neighborhood.\",\n",
    "        \"Heavy traffic discourages me from walking or biking.\",\n",
    "        \"Crossing the street feels unsafe with speeding cars.\",\n",
    "        \"The sidewalks are broken and uneven, difficult to walk on.\",\n",
    "        \"I avoid walking because there are no pedestrian crossings.\",\n",
    "        \"Poor street lighting makes me feel unsafe walking at night.\",\n",
    "        \"I have to walk in the road because there are no sidewalks.\",\n",
    "        \"The cars drive too fast and too close to pedestrians.\",\n",
    "        \"I always drive because it's not safe to walk here.\",\n",
    "        \"The intersection is dangerous for pedestrians and cyclists.\"\n",
    "    ]\n",
    "    \n",
    "    mixed_comments = [\n",
    "        \"Some parts of my route are nice, but others lack sidewalks.\",\n",
    "        \"The new crosswalks help, but drivers still go too fast.\",\n",
    "        \"I would walk more if the entire route was well-maintained.\",\n",
    "        \"Morning walk is pleasant, evening walk feels less safe.\",\n",
    "        \"Parts of the neighborhood are walkable, others are not.\",\n",
    "        \"The sidewalk network is incomplete in my area.\",\n",
    "        \"Some intersections are well-designed, others are dangerous.\",\n",
    "        \"Walking is okay when it's light out, but not after dark.\",\n",
    "        \"The bike lanes are good but don't connect to where I need to go.\",\n",
    "        \"Some drivers respect pedestrians, others don't.\"\n",
    "    ]\n",
    "    \n",
    "    texts = []\n",
    "    for i in range(n_samples):\n",
    "        if D[i] == 1 and Y[i] > 0.5:  # PFIP + high well-being\n",
    "            comment = np.random.choice(positive_comments)\n",
    "        elif D[i] == 1 and Y[i] <= 0.5:  # PFIP + moderate/low well-being\n",
    "            comment = np.random.choice(mixed_comments)\n",
    "        elif D[i] == 0 and Y[i] > -0.5:  # No PFIP + moderate well-being\n",
    "            comment = np.random.choice(mixed_comments)\n",
    "        else:  # No PFIP + low well-being\n",
    "            comment = np.random.choice(negative_comments)\n",
    "        texts.append(comment)\n",
    "    \n",
    "    return X, D, Y, images, gps_traces, texts\n",
    "\n",
    "# Generate synthetic data\n",
    "X, D, Y, images, gps_traces, texts = generate_synthetic_multimodal_data(n_samples=1000)\n",
    "\n",
    "# Create train-test split\n",
    "X_train, X_test, D_train, D_test, Y_train, Y_test, \\\n",
    "images_train, images_test, gps_train, gps_test, \\\n",
    "texts_train, texts_test = train_test_split(X, D, Y, images, gps_traces, texts, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f218372c",
   "metadata": {},
   "source": [
    "Let's visualize some of our synthetic data to get a better understanding of what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c30076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few example street view images\n",
    "fig, axs = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i in range(8):\n",
    "    idx = i\n",
    "    axs[i].imshow(images_train[idx])\n",
    "    axs[i].set_title(f\"PFIP: {D_train[idx]}, WB: {Y_train[idx]:.2f}\")\n",
    "    axs[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Example Synthetic Street View Images\", y=1.05, fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "# Display a few example text comments\n",
    "print(\"Example text comments:\")\n",
    "for i in range(5):\n",
    "    print(f\"PFIP: {D_train[i]}, WB: {Y_train[i]:.2f} - {texts_train[i]}\")\n",
    "\n",
    "# Visualize a GPS-accelerometer trace\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Sample traces for PFIP and non-PFIP areas\n",
    "pfip_idx = np.where(D_train == 1)[0][0]\n",
    "no_pfip_idx = np.where(D_train == 0)[0][0]\n",
    "\n",
    "# Plot coordinates\n",
    "axs[0].plot(gps_train[pfip_idx, :, 0], gps_train[pfip_idx, :, 1], 'o-', label='PFIP Area')\n",
    "axs[0].plot(gps_train[no_pfip_idx, :, 0], gps_train[no_pfip_idx, :, 1], 'o-', label='Non-PFIP Area')\n",
    "axs[0].set_title('Daily Movement Patterns')\n",
    "axs[0].set_xlabel('Latitude')\n",
    "axs[0].set_ylabel('Longitude')\n",
    "axs[0].legend()\n",
    "\n",
    "# Plot acceleration\n",
    "hours = np.arange(24)\n",
    "axs[1].plot(hours, gps_train[pfip_idx, :, 2], 'o-', label='PFIP Area')\n",
    "axs[1].plot(hours, gps_train[no_pfip_idx, :, 2], 'o-', label='Non-PFIP Area')\n",
    "axs[1].set_title('Daily Acceleration Patterns')\n",
    "axs[1].set_xlabel('Hour of Day')\n",
    "axs[1].set_ylabel('Acceleration')\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7552bfe6",
   "metadata": {},
   "source": [
    "## 2. Implementing the Three-Layer Architecture\n",
    "\n",
    "Now let's implement each component of the three-layer architecture (Φ → Ψ → Γ) for our active transportation setting.\n",
    "\n",
    "### 2.1 Representation Learning (Φ)\n",
    "\n",
    "First, we'll implement representation learners for each data modality. In practice, you would use pre-trained models or train custom deep learning models for each data type. For simplicity in this tutorial, we'll use simplified versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31756a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN for encoding street view images\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, embedding_dim=64):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(32 * 8 * 8, 128)  # For 32x32 input images\n",
    "        self.fc2 = nn.Linear(128, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1, 32 * 8 * 8)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Custom StreetviewEncoder that uses our CNN\n",
    "class CustomStreetviewEncoder(RepresentationLearner):\n",
    "    def __init__(self, embedding_dim=64):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.model = SimpleCNN(embedding_dim)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self._is_fitted = False\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # In a real implementation, we would train the model\n",
    "        # For this tutorial, we'll just pretend it's already trained\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Convert numpy array to PyTorch tensor\n",
    "        # X shape: (batch_size, height, width, channels)\n",
    "        X_tensor = torch.from_numpy(X).float().permute(0, 3, 1, 2)  # Change to (batch_size, channels, height, width)\n",
    "        X_tensor = X_tensor.to(self.device)\n",
    "        \n",
    "        # Get embeddings\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(X_tensor).cpu().numpy()\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Simple LSTM model for GPS traces\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=64, embedding_dim=64):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, input_dim)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # Take the output from the last time step\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        # Project to embedding dimension\n",
    "        embedding = self.fc(last_output)\n",
    "        return embedding\n",
    "\n",
    "# Custom GPS trace encoder\n",
    "class CustomGPSEncoder(RepresentationLearner):\n",
    "    def __init__(self, input_dim=3, hidden_dim=64, embedding_dim=64):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.model = SimpleLSTM(input_dim, hidden_dim, embedding_dim)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model.to(self.device)\n",
    "        self._is_fitted = False\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Pretend we've trained the model\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # X shape: (batch_size, seq_len, features)\n",
    "        X_tensor = torch.from_numpy(X).float()\n",
    "        X_tensor = X_tensor.to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(X_tensor).cpu().numpy()\n",
    "        \n",
    "        return embeddings\n",
    "\n",
    "# Simple text encoder (in a real application, you would use a pre-trained language model like BERT)\n",
    "class CustomTextEncoder(RepresentationLearner):\n",
    "    def __init__(self, embedding_dim=64):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self._is_fitted = False\n",
    "        # We'll use a simple bag-of-words approach for this tutorial\n",
    "        self.word_dict = {}\n",
    "    \n",
    "    def _text_to_bow(self, text):\n",
    "        \"\"\"Convert text to bag-of-words representation.\"\"\"\n",
    "        words = text.lower().replace('.', '').replace(',', '').split()\n",
    "        bow = np.zeros(len(self.word_dict))\n",
    "        for word in words:\n",
    "            if word in self.word_dict:\n",
    "                bow[self.word_dict[word]] = 1\n",
    "        return bow\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        # Build vocabulary from text data\n",
    "        vocab = set()\n",
    "        for text in X:\n",
    "            words = text.lower().replace('.', '').replace(',', '').split()\n",
    "            vocab.update(words)\n",
    "        \n",
    "        # Create word dictionary\n",
    "        self.word_dict = {word: i for i, word in enumerate(sorted(list(vocab)))}\n",
    "        \n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if not self._is_fitted:\n",
    "            raise ValueError(\"Model not fitted. Call fit() first.\")\n",
    "        \n",
    "        # Convert texts to bag-of-words\n",
    "        bow_matrix = np.array([self._text_to_bow(text) for text in X])\n",
    "        \n",
    "        # If vocabulary is larger than embedding dimension, use PCA to reduce dimensionality\n",
    "        if bow_matrix.shape[1] > self.embedding_dim:\n",
    "            # In a real implementation, use PCA or a neural network\n",
    "            # For simplicity, we'll just use a random projection\n",
    "            projection = np.random.randn(bow_matrix.shape[1], self.embedding_dim) / np.sqrt(self.embedding_dim)\n",
    "            embeddings = bow_matrix @ projection\n",
    "        elif bow_matrix.shape[1] < self.embedding_dim:\n",
    "            # Pad with zeros if vocabulary is smaller than embedding dimension\n",
    "            embeddings = np.pad(bow_matrix, ((0, 0), (0, self.embedding_dim - bow_matrix.shape[1])))\n",
    "        else:\n",
    "            embeddings = bow_matrix\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c3b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our custom encoders\n",
    "streetview_encoder = CustomStreetviewEncoder(embedding_dim=64)\n",
    "gps_encoder = CustomGPSEncoder(input_dim=3, embedding_dim=64)\n",
    "text_encoder = CustomTextEncoder(embedding_dim=64)\n",
    "\n",
    "# Train the encoders on our training data\n",
    "print(\"Training streetview encoder...\")\n",
    "streetview_encoder.fit(images_train)\n",
    "\n",
    "print(\"Training GPS encoder...\")\n",
    "gps_encoder.fit(gps_train)\n",
    "\n",
    "print(\"Training text encoder...\")\n",
    "text_encoder.fit(texts_train)\n",
    "\n",
    "# Create embeddings for each modality\n",
    "print(\"Creating embeddings...\")\n",
    "image_embeddings_train = streetview_encoder.transform(images_train)\n",
    "gps_embeddings_train = gps_encoder.transform(gps_train)\n",
    "text_embeddings_train = text_encoder.transform(texts_train)\n",
    "\n",
    "# Create a multimodal encoder that combines all three\n",
    "multimodal_encoder = MultiModalEncoder(\n",
    "    encoders={\n",
    "        'image': streetview_encoder,\n",
    "        'gps': gps_encoder,\n",
    "        'text': text_encoder\n",
    "    },\n",
    "    fusion_method='concatenate',\n",
    "    output_dim=128\n",
    ")\n",
    "\n",
    "# Fit the multimodal encoder\n",
    "print(\"Fitting multimodal encoder...\")\n",
    "multimodal_encoder.fit(\n",
    "    X={\n",
    "        'image': images_train,\n",
    "        'gps': gps_train,\n",
    "        'text': texts_train\n",
    "    }\n",
    ")\n",
    "\n",
    "# Generate latent representations\n",
    "Z_train = multimodal_encoder.transform(\n",
    "    X={\n",
    "        'image': images_train,\n",
    "        'gps': gps_train,\n",
    "        'text': texts_train\n",
    "    }\n",
    ")\n",
    "\n",
    "Z_test = multimodal_encoder.transform(\n",
    "    X={\n",
    "        'image': images_test,\n",
    "        'gps': gps_test,\n",
    "        'text': texts_test\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Created latent representations of shape {Z_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1153ef2",
   "metadata": {},
   "source": [
    "### 2.2 Balancing (Ψ)\n",
    "\n",
    "Next, we'll implement the balancing component that produces weights to equalize the treated and control distributions in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a02d1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a balancer\n",
    "balancer = KernelMMD(\n",
    "    kernel='rbf',\n",
    "    kernel_params={'gamma': 1/128},  # 1/dim is a common default\n",
    "    lambda_reg=0.01,\n",
    "    n_iterations=100  # Reduced for tutorial speed\n",
    ")\n",
    "\n",
    "# Fit the balancer on the latent representations\n",
    "print(\"Fitting balancer...\")\n",
    "balancer.fit(Z_train, D_train)\n",
    "\n",
    "# Generate balancing weights\n",
    "W_train = balancer.transform(Z_train, D_train)\n",
    "W_test = balancer.transform(Z_test, D_test)\n",
    "\n",
    "# Compute imbalance before and after weighting\n",
    "imbalance_before = balancer.measure_imbalance(Z_train, D_train)\n",
    "imbalance_after = balancer.measure_imbalance(Z_train, D_train, W_train)\n",
    "\n",
    "print(f\"Imbalance before weighting: {imbalance_before:.4f}\")\n",
    "print(f\"Imbalance after weighting: {imbalance_after:.4f}\")\n",
    "print(f\"Improvement: {100 * (1 - imbalance_after/imbalance_before):.2f}%\")\n",
    "\n",
    "# Visualize the weights\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(W_train[D_train == 0], bins=20, alpha=0.5, label='Control')\n",
    "plt.hist(W_train[D_train == 1], bins=20, alpha=0.5, label='Treated')\n",
    "plt.title('Distribution of Weights')\n",
    "plt.xlabel('Weight')\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "\n",
    "# Show the top principal components of Z before and after weighting\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "Z_train_pca = pca.fit_transform(Z_train)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(Z_train_pca[D_train == 0, 0], Z_train_pca[D_train == 0, 1], \n",
    "            s=W_train[D_train == 0] * 50, alpha=0.5, label='Control')\n",
    "plt.scatter(Z_train_pca[D_train == 1, 0], Z_train_pca[D_train == 1, 1], \n",
    "            s=W_train[D_train == 1] * 50, alpha=0.5, label='Treated')\n",
    "plt.title('PCA of Latent Space with Weights')\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd01c21",
   "metadata": {},
   "source": [
    "### 2.3 Causal Learning (Γ)\n",
    "\n",
    "Finally, we'll implement the causal learning component that estimates treatment effects using doubly robust methods with influence function corrections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed36bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the causal learner\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# Create outcome models (one for each treatment level)\n",
    "outcome_models = {\n",
    "    '0': RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42),\n",
    "    '1': RandomForestRegressor(n_estimators=100, min_samples_leaf=5, random_state=42)\n",
    "}\n",
    "\n",
    "# Create propensity model\n",
    "propensity_model = RandomForestClassifier(n_estimators=100, min_samples_leaf=5, random_state=42)\n",
    "\n",
    "# Initialize the doubly robust estimator\n",
    "causal_learner = DoublyRobust(\n",
    "    propensity_model=propensity_model,\n",
    "    outcome_models=outcome_models,\n",
    "    n_splits=3,  # Reduced for tutorial speed\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Fit the causal learner\n",
    "print(\"Fitting causal learner...\")\n",
    "causal_learner.fit(Z_train, D_train, Y_train, W_train)\n",
    "\n",
    "# Estimate the treatment effect\n",
    "effect = causal_learner.estimate(Z_test, D_test, Y_test, W_test)\n",
    "\n",
    "print(f\"\\nEstimated treatment effect (ATE): {effect['ate']:.4f}\")\n",
    "print(f\"Standard error: {effect['std_err']:.4f}\")\n",
    "print(f\"95% confidence interval: [{effect['conf_int'][0]:.4f}, {effect['conf_int'][1]:.4f}]\")\n",
    "\n",
    "# Get influence functions for individual examples\n",
    "influence_values = causal_learner.influence_function(Z_test, D_test, Y_test, W_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4206a429",
   "metadata": {},
   "source": [
    "### 2.4 Putting It All Together: The Complete Three-Layer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84b2e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the complete three-layer architecture\n",
    "three_layer_model = ThreeLayerArchitecture(\n",
    "    representation_learner=multimodal_encoder,\n",
    "    balancer=balancer,\n",
    "    causal_learner=causal_learner,\n",
    "    objective_lambda=1.0\n",
    ")\n",
    "\n",
    "# Fit the end-to-end model\n",
    "print(\"Fitting the complete three-layer architecture...\")\n",
    "three_layer_model.fit(\n",
    "    X={\n",
    "        'image': images_train,\n",
    "        'gps': gps_train,\n",
    "        'text': texts_train\n",
    "    },\n",
    "    D=D_train,\n",
    "    Y=Y_train\n",
    ")\n",
    "\n",
    "# Estimate the effect\n",
    "combined_effect = three_layer_model.estimate(\n",
    "    X={\n",
    "        'image': images_test,\n",
    "        'gps': gps_test,\n",
    "        'text': texts_test\n",
    "    },\n",
    "    D=D_test,\n",
    "    Y=Y_test\n",
    ")\n",
    "\n",
    "print(f\"\\nEstimated treatment effect from combined architecture: {combined_effect['ate']:.4f}\")\n",
    "print(f\"Standard error: {combined_effect['std_err']:.4f}\")\n",
    "print(f\"95% confidence interval: [{combined_effect['conf_int'][0]:.4f}, {combined_effect['conf_int'][1]:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65256b5c",
   "metadata": {},
   "source": [
    "## 3. Comparison with Standard Methods\n",
    "\n",
    "Let's compare our AI-augmented causal inference approach with standard methods that don't use the rich multimodal data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1013592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple difference in means\n",
    "simple_ate = np.mean(Y_test[D_test == 1]) - np.mean(Y_test[D_test == 0])\n",
    "\n",
    "# Linear regression adjustment\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X_D_test = np.column_stack([X_test, D_test.reshape(-1, 1)])\n",
    "linear_model = LinearRegression().fit(X_D_test, Y_test)\n",
    "linear_ate = linear_model.coef_[-1]\n",
    "\n",
    "# Random forest with observed covariates\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf_0 = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_test[D_test == 0], Y_test[D_test == 0])\n",
    "rf_1 = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_test[D_test == 1], Y_test[D_test == 1])\n",
    "Y_0_pred = rf_0.predict(X_test)\n",
    "Y_1_pred = rf_1.predict(X_test)\n",
    "rf_ate = np.mean(Y_1_pred - Y_0_pred)\n",
    "\n",
    "# Propensity score weighting with observed covariates\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "ps_model = LogisticRegression().fit(X_train, D_train)\n",
    "ps_test = ps_model.predict_proba(X_test)[:, 1]\n",
    "ps_weights = np.where(D_test == 1, 1/ps_test, 1/(1-ps_test))\n",
    "ps_weights = ps_weights / np.sum(ps_weights)\n",
    "ps_ate = np.sum(ps_weights * Y_test * (2*D_test - 1))\n",
    "\n",
    "# Compare results\n",
    "results = pd.DataFrame({\n",
    "    'Method': [\n",
    "        'Simple Difference in Means',\n",
    "        'Linear Regression',\n",
    "        'Random Forest',\n",
    "        'Propensity Score Weighting',\n",
    "        'AI-Augmented Causal Inference'\n",
    "    ],\n",
    "    'ATE Estimate': [\n",
    "        simple_ate,\n",
    "        linear_ate,\n",
    "        rf_ate,\n",
    "        ps_ate,\n",
    "        combined_effect['ate']\n",
    "    ],\n",
    "    'Standard Error': [\n",
    "        np.sqrt(np.var(Y_test[D_test == 1])/sum(D_test == 1) + np.var(Y_test[D_test == 0])/sum(D_test == 0)),\n",
    "        np.nan,  # Would need to compute\n",
    "        np.nan,  # Would need to compute\n",
    "        np.nan,  # Would need to compute\n",
    "        combined_effect['std_err']\n",
    "    ],\n",
    "    'Uses Multimodal Data': [\n",
    "        'No',\n",
    "        'No',\n",
    "        'No',\n",
    "        'No',\n",
    "        'Yes'\n",
    "    ]\n",
    "})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b2d704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['lightblue', 'lightblue', 'lightblue', 'lightblue', 'darkorange']\n",
    "bars = plt.bar(results['Method'], results['ATE Estimate'], color=colors)\n",
    "\n",
    "# Add error bars for methods with standard errors\n",
    "for i, method in enumerate(results['Method']):\n",
    "    if not np.isnan(results['Standard Error'].iloc[i]):\n",
    "        plt.errorbar(\n",
    "            i, \n",
    "            results['ATE Estimate'].iloc[i], \n",
    "            yerr=1.96 * results['Standard Error'].iloc[i],\n",
    "            fmt='none', color='black', capsize=5\n",
    "        )\n",
    "\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.title('Comparison of Causal Effect Estimates')\n",
    "plt.ylabel('Average Treatment Effect Estimate')\n",
    "plt.xticks(rotation=25, ha='right')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Highlight the multimodal method\n",
    "plt.annotate('Uses multimodal data', xy=(4, results['ATE Estimate'].iloc[4]), \n",
    "             xytext=(4, results['ATE Estimate'].iloc[4] + 0.1),\n",
    "             ha='center', va='bottom',\n",
    "             bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.5),\n",
    "             arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.2'))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799e9be6",
   "metadata": {},
   "source": [
    "## 4. Using ActiveBERTDML for An End-to-End Solution\n",
    "\n",
    "Finally, let's demonstrate the use of the ActiveBERTDML workflow as described in Section 5.5 of the paper, which provides an end-to-end solution for active transportation research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecab8114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ActiveBERTDML\n",
    "active_bert_dml = ActiveBERTDML(\n",
    "    image_encoder=CustomStreetviewEncoder(embedding_dim=64),\n",
    "    text_encoder=CustomTextEncoder(embedding_dim=64),\n",
    "    balancer=KernelMMD(kernel='rbf', n_iterations=100),\n",
    "    fusion_method='concatenate',\n",
    "    latent_dim=128\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "print(\"Fitting ActiveBERTDML model...\")\n",
    "active_bert_dml.fit(\n",
    "    images=images_train,\n",
    "    texts=texts_train,\n",
    "    D=D_train,\n",
    "    Y=Y_train\n",
    ")\n",
    "\n",
    "# Estimate the effect\n",
    "active_bert_effect = active_bert_dml.estimate(\n",
    "    images=images_test,\n",
    "    texts=texts_test,\n",
    "    D=D_test,\n",
    "    Y=Y_test\n",
    ")\n",
    "\n",
    "print(f\"\\nEstimated treatment effect from ActiveBERTDML: {active_bert_effect['ate']:.4f}\")\n",
    "print(f\"Standard error: {active_bert_effect['std_err']:.4f}\")\n",
    "print(f\"95% confidence interval: [{active_bert_effect['conf_int'][0]:.4f}, {active_bert_effect['conf_int'][1]:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bfb718",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this tutorial, we've demonstrated the implementation of AI-augmented causal inference for active transportation research. The three-layer architecture (Φ → Ψ → Γ) provides a powerful framework for estimating causal effects from multimodal data, leveraging advanced machine learning techniques while maintaining rigorous statistical guarantees.\n",
    "\n",
    "We've shown how to:\n",
    "\n",
    "1. Implement representation learners for different data modalities (street views, GPS traces, text)\n",
    "2. Use balancing methods to ensure covariate overlap in the latent space\n",
    "3. Apply doubly robust causal estimators with influence function corrections\n",
    "4. Combine these components into an end-to-end AI-augmented causal inference pipeline\n",
    "\n",
    "The results demonstrate that incorporating rich, high-dimensional data can lead to more accurate and nuanced causal effect estimates compared to standard methods that rely only on tabular covariates.\n",
    "\n",
    "This approach is particularly valuable for active transportation research, where the built environment, mobility patterns, and subjective experiences all contribute to the causal pathways linking infrastructure interventions to travel behavior and well-being outcomes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
